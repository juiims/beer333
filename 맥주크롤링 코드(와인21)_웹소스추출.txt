# a는 마지막 페이지번호를 입력
def source_crawler(page):
    # 필요한 라이브러리 임포트
    from bs4 import BeautifulSoup
    from urllib.request import urlopen, Request

    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.by import By

    from bs4 import BeautifulSoup
    import time

    # selenium을 이용하여 웹페이지 소스 긁어오기
    chrome_driver = 'C:/Users/JIn/Documents/chromedriver' 

    # 크롤링할 url
    target_url = 'https://www.wine21.com/13_search/beer_list.html#List'
    
    page = page-1
    soup = []
    
    # 첫 번째 페이지 열기
    driver = webdriver.Chrome(chrome_driver)
    driver.get(target_url)

    # 목록 수 100개 설정 
    driver.find_element_by_xpath("//select/option[@value='100']").click()

    # 페이지소스 저장
    src = driver.page_source
    soup.append(BeautifulSoup(src))
    
    driver.close()
        
    for i in range(page):

        # 페이지 열기
        driver = webdriver.Chrome(chrome_driver)
        driver.get(target_url)

        # 목록 수 100개 설정 
        driver.find_element_by_xpath("//select/option[@value='100']").click()

        # 페이지 전환
        page = driver.find_elements_by_xpath('//div[@class="pagenate ver_web"]/a')
        page[i].click()

        # 페이지소스 저장
        src = driver.page_source
        soup.append(BeautifulSoup(src))

        driver.close()

    return soup